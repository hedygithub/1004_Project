{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cd <your path>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q6Pw0pJSBBv"
   },
   "source": [
    "# Begining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7v4rsrRYIp0"
   },
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43991,
     "status": "ok",
     "timestamp": 1620761332969,
     "user": {
      "displayName": "Di He",
      "photoUrl": "",
      "userId": "01551765020394733104"
     },
     "user_tz": 240
    },
    "id": "RtxMt53pSBBw",
    "outputId": "f63d06f5-1d2a-416d-d4bb-a4ba74e36c99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
      "\u001b[K     |████████████████████████████████| 212.3MB 26kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 38.2MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=5663394ce13b9f2efc776998c3c4f00158e4fe1b17c2290fa84539645450b350\n",
      "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9 pyspark-3.1.1\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "di0517yhSBBx"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "\n",
    "from operator import itemgetter\n",
    "import time\n",
    "import sys\n",
    "import getpass\n",
    "from pathlib import Path\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKUNH8c4SBBy"
   },
   "source": [
    "## Define the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91U9WtARSBBy"
   },
   "outputs": [],
   "source": [
    "PATH = './data'\n",
    "\n",
    "WHOLE_DATA = True\n",
    "FRACTION = 1.00\n",
    "MODEL_PATH = 'saved_tuned_best_model_{}'.format(str(FRACTION).replace('.','_'))\n",
    "STATS_PATH = Path('./saved_tuned_model_stats')\n",
    "\n",
    "TOP=500\n",
    "PREC_AT = 500\n",
    "\n",
    "\n",
    "MAX_MEMORY = \"4g\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('quq') \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QlF_Ni1foiu"
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1085,
     "status": "ok",
     "timestamp": 1620771055101,
     "user": {
      "displayName": "Di He",
      "photoUrl": "",
      "userId": "01551765020394733104"
     },
     "user_tz": 240
    },
    "id": "6XHifVcqdI-9",
    "outputId": "d2174d20-6792-4b91-e6b3-a87ae73208f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the data\n"
     ]
    }
   ],
   "source": [
    "if WHOLE_DATA:\n",
    "    train = spark.read.parquet(PATH+'/cf_train_trans.parquet')\n",
    "    unique_user_index_val = spark.read.parquet(PATH+'/unique_user_index_val.parquet')\n",
    "else:\n",
    "    train = spark.read.parquet(PATH+'/cf_train_trans_{}.parquet'.format(str(FRACTION).replace('.','_')))\n",
    "    unique_user_index_val = spark.read.parquet(PATH+'/unique_user_index_val.parquet')\n",
    "    unique_user_index_val = unique_user_index_val.sample(withReplacement=False, fraction=max(FRACTION,0.05))\n",
    "\n",
    "true_rec_tracks_val_rdd = sc.textFile(PATH+'/true_rec_tracks_val').map(eval)\n",
    "print('Successfully loaded the data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpoiMQzdjUKs"
   },
   "source": [
    "# Tune ALS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3991778,
     "status": "error",
     "timestamp": 1620775050072,
     "user": {
      "displayName": "Di He",
      "photoUrl": "",
      "userId": "01551765020394733104"
     },
     "user_tz": 240
    },
    "id": "Ub8i465z6tr2",
    "outputId": "796c81e9-b352-4106-e9d4-248f1f141cec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of params 4\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py -f /root/.local/share/jupyter/runtime/kernel-da9016d2-1942-4739-8ca3-5b76bff19d93.json\n",
      "With Rank:100, Reg:1, Alpha:1, Maxiter:5, Metric: 0.0093638, time TTL: 3787.433854341507, fit: 2855.6211857795715, rec: 6.690388917922974, evl: 925.1222796440125\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c860a8d8c362>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m               \u001b[0mimplicitPrefs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoldStartStrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"drop\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m               rank=rk, regParam=reg, alpha=alp, maxIter=it)\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mtime_to_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msince\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o704.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 734.0 failed 1 times, most recent failure: Lost task 1.0 in stage 734.0 (TID 4861) (b5313f649591 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2066)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2102)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:188)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:116)\n\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$2(CoGroupedRDD.scala:140)\n\tat org.apache.spark.rdd.CoGroupedRDD$$Lambda$2842/0x00000008411ec040.apply(Unknown Source)\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n\tat scala.collection.TraversableLike$WithFilter$$Lambda$167/0x00000008401c9840.apply(Unknown Source)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2297)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1209)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1202)\n\tat org.apache.spark.ml.recommendation.ALS$.computeYtY(ALS.scala:1765)\n\tat org.apache.spark.ml.recommendation.ALS$.computeFactors(ALS.scala:1687)\n\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$train$8(ALS.scala:1011)\n\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$train$8$adapted(ALS.scala:998)\n\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:998)\n\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:709)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:691)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2066)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2102)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:188)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:116)\n\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$2(CoGroupedRDD.scala:140)\n\tat org.apache.spark.rdd.CoGroupedRDD$$Lambda$2842/0x00000008411ec040.apply(Unknown Source)\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n\tat scala.collection.TraversableLike$WithFilter$$Lambda$167/0x00000008401c9840.apply(Unknown Source)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n"
     ]
    }
   ],
   "source": [
    "ranks = [i for i in range(100,200,30)]  \n",
    "# ranks = [i for i in range(190,300,30)]\n",
    "regParams = [1] #[10**i for i in range(-2,1)   \n",
    "alphas = [1]\n",
    "maxIter = [5]\n",
    "\n",
    "params = [[a,b,c,d] for a,b,c,d in itertools.product(ranks, regParams, alphas, maxIter)]\n",
    "print('length of params',len(params))\n",
    "\n",
    "precisionAt_k_highest = 0\n",
    "stats_file = open(STATS_PATH/'stats_{}.txt'.format(str(FRACTION).replace('.','_')), 'a', buffering=1)\n",
    "print(' '.join(sys.argv))\n",
    "print(' '.join(sys.argv), file=stats_file)\n",
    "\n",
    "\n",
    "for rk,reg,alp,it in itertools.product(ranks, regParams, alphas, maxIter):\n",
    "    since = time.time()\n",
    "\n",
    "    als = ALS(userCol='user_index', itemCol='track_index', ratingCol='count', \n",
    "              implicitPrefs=True, coldStartStrategy=\"drop\",\n",
    "              rank=rk, regParam=reg, alpha=alp, maxIter=it)\n",
    "    model = als.fit(train)\n",
    "    time_to_fit = time.time() - since\n",
    "\n",
    "    userRecs = model.recommendForUserSubset(unique_user_index_val, TOP)\n",
    "    pred_rec_tracks_val_rdd = userRecs.rdd.map(lambda row: (row['user_index'], \n",
    "                                                            [track_pred.track_index for track_pred in row['recommendations']]))\n",
    "\n",
    "    pred_and_true_tracks = pred_rec_tracks_val_rdd.join(true_rec_tracks_val_rdd).map(lambda tup: tup[1])\n",
    "    time_to_recommend = time.time() - since\n",
    "\n",
    "    metrics = RankingMetrics(pred_and_true_tracks)\n",
    "    precisionAt_k = metrics.precisionAt(PREC_AT)\n",
    "    time_to_eval = time.time() - since\n",
    "    if precisionAt_k > precisionAt_k_highest:\n",
    "        precisionAt_k_highest = precisionAt_k\n",
    "        model.write().overwrite().save(MODEL_PATH)\n",
    "    \n",
    "    print(\"With Rank:{}, Reg:{}, Alpha:{}, Maxiter:{}, Metric: {}, time TTL: {}, fit: {}, rec: {}, evl: {}\".format(\n",
    "        rk,reg,alp,it, \n",
    "        precisionAt_k, \n",
    "        time_to_eval, time_to_fit, time_to_recommend-time_to_fit, time_to_eval-time_to_recommend \n",
    "    ), file=stats_file)\n",
    "\n",
    "    print(\"With Rank:{}, Reg:{}, Alpha:{}, Maxiter:{}, Metric: {}, time TTL: {}, fit: {}, rec: {}, evl: {}\".format(\n",
    "        rk,reg,alp,it, \n",
    "        precisionAt_k, \n",
    "        time_to_eval, time_to_fit, time_to_recommend-time_to_fit, time_to_eval-time_to_recommend \n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chkEoxIdUqA1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "02_tune_model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
